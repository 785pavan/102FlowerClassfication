{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "import scipy.io\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-16 18:54:17--  http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
      "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
      "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 344862509 (329M) [application/x-gzip]\n",
      "Saving to: ‘102flowers.tgz’\n",
      "\n",
      "102flowers.tgz      100%[===================>] 328.89M  86.4MB/s    in 4.0s    \n",
      "\n",
      "2019-12-16 18:54:21 (83.0 MB/s) - ‘102flowers.tgz’ saved [344862509/344862509]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-16 18:54:21--  http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
      "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
      "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 502\n",
      "Saving to: ‘imagelabels.mat’\n",
      "\n",
      "imagelabels.mat     100%[===================>]     502  --.-KB/s    in 0s      \n",
      "\n",
      "2019-12-16 18:54:21 (99.3 MB/s) - ‘imagelabels.mat’ saved [502/502]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = scipy.io.loadmat('imagelabels.mat')\n",
    "labels = np.array(labels['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFile  = tarfile.open(name='102flowers.tgz', mode='r:gz')\n",
    "\n",
    "savepath = '/tmp/102flowers/'\n",
    "\n",
    "imageFile.extractall(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/tmp/train/'\n",
    "test_path = '/tmp/test/'\n",
    "if not os.path.isdir(train_path):\n",
    "  os.mkdir(train_path)\n",
    "if not os.path.isdir(test_path):\n",
    "  os.mkdir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 %\n"
     ]
    }
   ],
   "source": [
    "count_M20 = 0\n",
    "for i in range(101):\n",
    "    rand = np.random.randint(0, 100, dtype= np.int64)\n",
    "    if rand > 20:\n",
    "        count_M20 += 1\n",
    "print(count_M20, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f399806254b47edafd346a2d09ee2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8189), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = savepath + 'jpg/'\n",
    "np.random.seed(42)\n",
    "for f in tqdm_notebook(os.listdir(path), total= len(os.listdir(path))):\n",
    "  image = Image.open(os.path.join(path, f))\n",
    "  image_id = int(f.replace('image_', \"\").replace('.jpg',\"\"))\n",
    "  rand = np.random.randint(0,100,dtype= np.int64)\n",
    "  if rand > 20:\n",
    "    if not os.path.isdir(train_path + str(labels[image_id-1])):\n",
    "        os.mkdir(train_path + str(labels[image_id-1]))\n",
    "    image.save(train_path + str(labels[image_id-1]) + '/' + f )\n",
    "  else:\n",
    "    if not os.path.isdir(test_path + str(labels[image_id-1])):\n",
    "        os.mkdir(test_path + str(labels[image_id-1]))\n",
    "    image.save(test_path + str(labels[image_id-1]) + '/' + f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5071 images belonging to 102 classes.\n",
      "Found 1361 images belonging to 102 classes.\n",
      "Found 1757 images belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class FixedImageDataGenerator(ImageDataGenerator):\n",
    "    def standardize(self, x):\n",
    "        if self.featurewise_center:\n",
    "            x = ((x/255.) - 0.5) * 2.\n",
    "        return x\n",
    "    \n",
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=90,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     zoom_range=0.2,\n",
    "                     rescale = 1/255.,\n",
    "                     validation_split=0.22)\n",
    "\n",
    "#train_datagen = ImageDataGenerator(rescale = 1/255., validation_split=0.2)\n",
    "\n",
    "train_datagen = FixedImageDataGenerator(**data_gen_args)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_path, subset='training', target_size = (500,500), batch_size = 16, class_mode = 'categorical', seed=42, shuffle=True)\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(train_path, subset='validation', target_size = (500,500), batch_size = 16, class_mode = 'categorical', seed=42, shuffle=True)\n",
    "\n",
    "#train_generator = train_datagen.flow(imgs, y_one_hot, batch_size=128, subset='training', seed=42)\n",
    "\n",
    "test_datagen = FixedImageDataGenerator(rescale= 1/255.)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_path, target_size= (500,500), batch_size = 16, class_mode = 'categorical', seed = 42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT, CHANNELS = (500,500,3)\n",
    "eps = 1.1e-5\n",
    "concat_axis = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "try:\n",
    "    from tensorflow.keras import initializations\n",
    "except ImportError:\n",
    "    from tensorflow.keras import initializers as initializations\n",
    "\n",
    "class Scale(Layer):\n",
    "    '''Custom Layer for DenseNet used for BatchNormalization.\n",
    "    \n",
    "    Learns a set of weights and biases used for scaling the input data.\n",
    "    the output consists simply in an element-wise multiplication of the input\n",
    "    and a sum of a set of constants:\n",
    "        out = in * gamma + beta,\n",
    "    where 'gamma' and 'beta' are the weights and biases learnt.\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "    '''\n",
    "    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.initial_weights = weights\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (int(input_shape[self.axis]),)\n",
    "\n",
    "        self.gamma = tf.Variable(self.gamma_init(shape), name='{}_gamma'.format(self.name))\n",
    "        self.beta = tf.Variable(self.beta_init(shape), name='{}_beta'.format(self.name))\n",
    "        #self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        out = tf.reshape(self.gamma, broadcast_shape) * x + tf.reshape(self.beta, broadcast_shape)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
    "        base_config = super(Scale, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def conv_block(x, stage, branch, nb_filter, dropout_rate = None, weight_decay = 1e-4):\n",
    "  conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
    "  relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
    "  #1X1 convolution\n",
    "  inter_channel = nb_filter * 4\n",
    "  x = BatchNormalization(epsilon=eps, axis=concat_axis, name = conv_name_base + '_x1_bn')(x)\n",
    "  x = Scale(axis=concat_axis, name=conv_name_base + '_x1_scale')(x)\n",
    "  x = Activation('relu', name= relu_name_base + '_x1')(x)\n",
    "  x = Conv2D(inter_channel, (1,1), name=conv_name_base + '_x1', use_bias=False)(x)\n",
    "  \n",
    "  if dropout_rate:\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "  #3X3 convolution\n",
    "  x = BatchNormalization(epsilon=eps, axis= concat_axis, name=conv_name_base + '_x2_bn')(x)\n",
    "  x = Scale(axis= concat_axis, name = conv_name_base + '_x2_scale')(x)\n",
    "  x = Activation('relu', name= relu_name_base + '_x2')(x)\n",
    "  x = ZeroPadding2D((1,1), name= conv_name_base + '_x2_zeropadding')(x)\n",
    "  x = Conv2D(nb_filter, (3,3), name=conv_name_base + '_x2', use_bias=False)(x)\n",
    "\n",
    "  if dropout_rate:\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "  \n",
    "  return x\n",
    "\n",
    "def transition_block(x, stage, nb_filter, compression= 1.0, dropout_rate = None, weight_decay = 1e-4):\n",
    "  conv_name_base = 'conv' + str(stage) + '_blk'\n",
    "  relu_name_base = 'relu' + str(stage) + '_blk'\n",
    "  pool_name_base = 'pool' + str(stage)\n",
    "  x = BatchNormalization(epsilon=eps, axis = concat_axis, name=conv_name_base + '_bn')(x)\n",
    "  x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
    "  x = Activation('relu', name= relu_name_base)(x)\n",
    "  x = Conv2D(int(nb_filter * compression), (1,1), name= conv_name_base, use_bias=False)(x)\n",
    "  if dropout_rate:\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "  x = AveragePooling2D((2,2), strides=(2,2), name = pool_name_base)(x)\n",
    "  return x\n",
    "\n",
    "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay= 1e-4, grow_nb_filters= True):\n",
    "  concat_feat = x\n",
    "  for i in range(nb_layers):\n",
    "    branch = i + 1\n",
    "    x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
    "    concat_feat = concatenate([concat_feat, x], axis=concat_axis, name = 'concat_'+str(stage)+'_'+str(branch))\n",
    "\n",
    "    if grow_nb_filters:\n",
    "      nb_filter += growth_rate\n",
    "  return concat_feat, nb_filter\n",
    "\n",
    "def DenseNet(nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.0, dropout_rate=0.3, weight_decay=1e-4, classes=1000, weights_path=None):\n",
    "  compression = 1.0 - reduction\n",
    "  nb_layers = [2,3,6,4] #[6,12,24,16]\n",
    "  #input layer\n",
    "  img_input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS), name='data')\n",
    "  #initial convolution\n",
    "  x = ZeroPadding2D((3,3), name='conv1_zeropadding')(img_input)\n",
    "  x = Conv2D(nb_filter, (7,7), strides=(2,2), name = 'conv1', use_bias=False )(x)\n",
    "  x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv1_bn')(x)\n",
    "  x = Scale(axis= concat_axis, name='conv1_scale')(x)\n",
    "  x = Activation(activation='relu', name= 'relu1')(x)\n",
    "  x = ZeroPadding2D((1,1), name='pool1_zeropadding')(x)\n",
    "  x = MaxPool2D((3,3), strides=(2,2), name= 'pool1')(x)\n",
    "\n",
    "  #Adding dense blocks\n",
    "  for i in range(nb_dense_block - 1):\n",
    "    stage = i + 2\n",
    "    x, nb_filter = dense_block(x, stage, nb_layers[i], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    #transition block\n",
    "    x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "    nb_filter = int(nb_filter * compression)\n",
    "  final_stage = stage + 1\n",
    "  x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate,  weight_decay=weight_decay)\n",
    "  x = BatchNormalization(epsilon=eps, axis = concat_axis, name='conv'+str(final_stage)+'_blk_bn')(x)\n",
    "  x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
    "  x = Activation('relu', name = 'relu' + str(final_stage) +'_blk' )(x)\n",
    "  x = GlobalAveragePooling2D(name='pool' + str(final_stage) +'_blk')(x)\n",
    "  x = Dense(classes, name='fc6')(x)\n",
    "  x = Activation('softmax', name='prob')(x)\n",
    "  model = Model(img_input, x, name='densenet')\n",
    "  if weights_path is not None:\n",
    "    model.load_weight(weights_path)\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DenseNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a6ce6c6f82f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDenseNetModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m102\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DenseNet' is not defined"
     ]
    }
   ],
   "source": [
    "DenseNetModel = DenseNet(classes=102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fn = tf.optimizers.schedules.PiecewiseConstantDecay([100, 150, 200], [0.1, 0.05, 0.005, 0.0005])\n",
    "DenseNetModel.compile(optimizer= tf.optimizers.Adam(0.0001), loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "class mycallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy')>.95):\n",
    "            print(\"\\nReached 95% accuracy!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "myCallback = mycallback()\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "             tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1), myCallback,\n",
    "             tf.keras.callbacks.ModelCheckpoint('densenet_flower_class_mark03.h5', verbose=1, save_best_only=True, save_weights_only=True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/input/training01/'+checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 13961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = DenseNetModel.fit_generator(train_generator, steps_per_epoch=2*(train_generator.samples//train_generator.batch_size), validation_data=valid_generator,validation_steps=valid_generator.samples//valid_generator.batch_size , callbacks=callbacks, verbose=1, epochs=50, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNetModel.load_weights('densenet_flower_class_mark03.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predict = DenseNetModel.predict_generator(test_generator,steps = nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNetModel.evaluate_generator(test_generator,steps = nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
